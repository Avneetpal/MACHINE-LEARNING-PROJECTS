{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPU3l3LmYDPI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "CAEeCzVdYY2k",
        "outputId": "790b89f3-7dad-4a4b-c935-fd3b6eab78c1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2c349cc4-fa3b-4d32-b636-a8ab7727b887\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2c349cc4-fa3b-4d32-b636-a8ab7727b887\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "path_to_file=list(files.upload().keys())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ydz70JpGYhxb"
      },
      "outputs": [],
      "source": [
        "with open(path_to_file,'r', encoding='utf-8') as file:\n",
        "  text=file.read()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D_xfrWLYyNM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ofn_fGhEY46p"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbq4AwpNZH8-"
      },
      "outputs": [],
      "source": [
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words=len(tokenizer.word_index)+1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdTYzjVzajFU"
      },
      "source": [
        "In the above code, the text is tokenized, which means it is divided into individual words or tokens. The Tokenizer' object is created, which will handle the tokenization process. The 'fit_on_texts' method of the tokenizer is called, passing the 'text' as input. This method analyzes the text and builds a vocabulary of unique words, assigning each word a numerical index. The 'total_words' variable is then assigned the value of the length of the word index plus one, representing the total number of distinct words in the text.In the above code, the text is tokenized, which means it is divided into individual words or tokens. The Tokenizer' object is created, which will handle the tokenization process. The 'fit_on_texts' method of the tokenizer is called, passing the 'text' as input. This method analyzes the text and builds a vocabulary of unique words, assigning each word a numerical index. The 'total_words' variable is then assigned the value of the length of the word index plus one, representing the total number of distinct words in the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBryFfjAbICn"
      },
      "source": [
        "now lets create input - output pairs by splitting the text into sequence of tokens and forming  n-grams form the sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IPgB01mTbemJ"
      },
      "outputs": [],
      "source": [
        "input_sequences=[]\n",
        "for line in text.split('\\n'):\n",
        "  token_list=tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1,len(token_list)):\n",
        "    n_gram_sequence=token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GF2wvWj9eFhs"
      },
      "outputs": [],
      "source": [
        "max_sequence_len=max([len(seq) for seq in input_sequences])\n",
        "input_sequences=np.array(pad_sequences(input_sequences,maxlen=max_sequence_len,padding='pre'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIQsUF_eecgs"
      },
      "source": [
        "In the above code, the input sequences are padded to ensure all sequences have the same length.\n",
        "The variable 'max_sequence_len' is assigned the maximum length among all the input sequences.\n",
        "The 'pad_sequences' function is used to pad or truncate the input sequences to match this maximum length.\n",
        "The 'pad_sequences' function takes the input_sequences list, sets the maximum length to 'max sequence_len',\n",
        "and specifies that the padding should be added at the beginning of each sequence using the 'padding=pre' argument. Finally,\n",
        "the input sequences are converted into a numpy array to facilitate further processing.\n",
        " Now let's split the sequences into input and output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0nAT6_neoQu"
      },
      "outputs": [],
      "source": [
        "x=input_sequences[:,:-1]\n",
        "y=input_sequences[:,:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FR3jHkEfeyqR"
      },
      "outputs": [],
      "source": [
        " y= np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt-5U1dIe-L3"
      },
      "source": [
        " In the above code, the input sequences are split into two arrays, 'X' and 'y', to create the input and output for training the next word prediction model. The 'X' array is assigned the values of all rows in the 'input_sequences' array except for the last column. It means that 'X' contains all the tokens in each sequence except for the last one, representing the input context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jNeCubWe4UM"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "model = Sequential()\n",
        "model.add(Embedding (total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0x4tC8-f7gA"
      },
      "source": [
        " The code above defines the model architecture for the next word prediction model. The 'Sequential' model is created, which represents a linear stack of layers.\n",
        " The first layer added to the model is the 'Embedding' layer, which is responsible for converting the input sequences into dense vectors of fixed size. It takes three arguments:\n",
        " (1). 'total_words', which represents the total number of distinct words in the vocabulary;\n",
        " (2). '100', which denotes the dimensionality of the word embeddings;\n",
        " (3). and 'input length', which specifies the length of the input sequences.\n",
        " The next layer added is the 'LSTM' layer, a type of recurrent neural network (RNN) layer designed for capturing sequential dependencies in the data.\n",
        " It has 150 units, which means it will learn 150 internal representations or memory cells.\n",
        "\n",
        " Finally, the 'Dense' layer is added, which is a fully connected layer that produces the output predictions.\n",
        " It has 'total_words' units and uses the 'softmax' activation function to convert the predicted scores into probabilities, indicating\n",
        " the likelihood of each word being the next one in the sequence.\n",
        " Now let's compile and train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPpkyYk-gft0"
      },
      "outputs": [],
      "source": [
        "model. compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=100, verbose=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtFGBzqorJmQ"
      },
      "source": [
        "In the above code, the model is being compiled and trained. The 'compile' method configures the model for training. The 'loss' parameter is set\n",
        "to 'categorical_crossentropy', a commonly used loss function for multi-class classification problems. The 'optimizer' parameter is set to 'adam',\n",
        "an optimization algorithm that adapts the learning rate during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX8HcuhmrPUs"
      },
      "source": [
        "The 'metrics' parameter is set to 'accuracy' to monitor the accuracy during training. After compiling the model, the 'fit' method is called to train\n",
        "the model on the input sequences 'X' and the corresponding output 'y'. The 'epochs' parameter specifies the number of times the training\n",
        "process will iterate over the entire dataset. The 'verbose' parameter is set to '1' to display the training process.\n",
        "The above code will take more than an hour to execute. Once the code is executed, here's how we can generate the next word predictions using\n",
        "our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yemHivGhrP-e"
      },
      "outputs": [],
      "source": [
        "# seed_text = \"I will leave if they\"\n",
        "seed_text = \"Are you\"\n",
        "next_words = 3\n",
        "\n",
        "for _ in range(next_words):\n",
        "token_list = tokenizer.texts_to_sequences([seed_text])[ø]\n",
        "token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "predicted = np.argmax(model.predict(token_list), axis =- 1)\n",
        "output_word = \"\"\n",
        "for word, index in tokenizer.word_index.items():\n",
        "if index == predicted:\n",
        "output_word = word\n",
        "break\n",
        "seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}